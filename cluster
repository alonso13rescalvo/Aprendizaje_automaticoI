

### Aprendizaje no supervisado

```{r}


library(tidyverse)  #  manipilación de datos
library(cluster)    # algoritmos de clustering 
library(factoextra) # algoritmos de clustering & visualización

continuas_train <-na.omit(train[, c("age","avg_glucose_level", "bmi")])
#continuas_train <-na.omit(PC1)
df <- continuas_train
# Reproducible
# Matriz de disimilaridades
d <- dist(scale(df), method = "euclidean")
# la distancia euclidiana es una métrica estándar, en teoría ya cumple todas las condiciones.

distancia <- as.matrix(d)
x <- 2
y <- 3
z <- 5
distancia[x,y] >=0
all(diag(distancia) == 0)
distancia[x,y] == distancia[y,x]
print(distancia[x,z] <= distancia[x,y] + distancia[y,z])
#Metodo del codo para saber cuantos cluster hacer
fviz_nbclust(scale(df), kmeans, method = "wss")


#Clustering no jerarquico
k3 <- kmeans(scale(df), centers = 3, nstart = 20)
fviz_cluster(k3, data = scale(df))

# Obtener los centroides
centroides <- k3$centers[k3$cluster, ]

# Calcular distancias euclidianas de cada punto a su centroide
distancias <- sqrt(rowSums((scale(df) - centroides)^2))

# Ver las distancias más altas (posibles outliers)
outliers <- order(distancias, decreasing = TRUE)[1:2]  # Tomamos los 2 más alejados
print(outliers)
df_limpio <- df[-outliers, ]  # Eliminar filas con outliers
k3_limpio <- kmeans(scale(df_limpio), centers = 3, nstart = 20)
fviz_cluster(k3_limpio, data = scale(df_limpio))  # Visualizar el nuevo clustering


#metodo silueta
fviz_nbclust(df, kmeans, method = "silhouette")
sil <- silhouette(k3$cluster, dist(df))
fviz_silhouette(sil)




df %>%
  pivot_longer(cols = -cluster, names_to = "variable", values_to = "valor") %>%
  ggplot(aes(x = factor(cluster), y = valor, fill = factor(cluster))) +
  geom_boxplot() +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal()
```
Al utilizar la función dist() de R , la matriz de distancias que estamos generando cumple las condiciones para ser una medida de desemejanza


### Clustering jerárquico
```{r}
# Método de Ward
hc5 <- hclust(d, method = "ward.D2" )

# Cortamos en 3 clusters
sub_grp <- cutree(hc5, k = 3)

# Visualizamos el corte en el dendrograma
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 3, border = 2:5)

# Número de observaciones en cada cluster
table(sub_grp)
# Visualización
fviz_cluster(list(data=scale(df),cluster=sub_grp))


```
Como podemos observar en ambos cluster, tanto jerarquico como no jerarquico podemos ver dos datos que parecen outliers , ero la separacion de cluster es muy pareja.

